{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrco2MGsxePJ"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaEGEKhpxaQD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCzGnap6vTiC"
   },
   "source": [
    "# Model performance plotting for different models (brainstorm ideas)\n",
    "1.   Consider different and suitable metrics specific to a model\n",
    "2.   Consider some default metrics if not provided\n",
    "3.   Define helper functions as needed\n",
    "4.   Clarify each function parameter with comments\n",
    "5.   Probably best to generate multiple plots\n",
    "6.   We will start with functions. Maybe later we can turn them into classes.\n",
    "7.   Keep in mind users will preprocess their datasets before calling our functions, so we won't include preprocessing steps in our functions. We will describe to them how we expect them to preprocess their datasets.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXnYHDkK0xDL"
   },
   "source": [
    "Time Series Forcasting related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbalmn4wvkao"
   },
   "outputs": [],
   "source": [
    "def UniTimeSeriesPlotPerformance(model, X_test, y_test, metrics):\n",
    "  \"\"\"\n",
    "  Generate plots with different metrics to evaluate model performance.\n",
    "  :param model: the trained univariate time series model\n",
    "  :type model: object\n",
    "\n",
    "  :param X_test: the test dataset containing only the features\n",
    "  :type X_test: pandas DataFrame\n",
    "\n",
    "  :param y_test: the test dataset containing only the target\n",
    "  :type y_test: pandas DataFrame\n",
    "\n",
    "  :param metrics: different metrics for plotting\n",
    "  :type metrics: str or list of strs\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pRqr7LcyGYd"
   },
   "outputs": [],
   "source": [
    "def MultiTimeSeriesPlotPerformance(y_pred, y_test, metrics):\n",
    "  \"\"\"\n",
    "  Generate plots with different metrics to evaluate model performance.\n",
    "  :param model: the trained multivariate time series model with predict() method\n",
    "  :type model: object\n",
    "\n",
    "  :param X_test: the test dataset containing only the features\n",
    "  :type X_test: pandas DataFrame\n",
    "\n",
    "  :param y_test: the test dataset containing only the target\n",
    "  :type y_test: pandas DataFrame\n",
    "\n",
    "  :param metrics: different metrics for plotting\n",
    "  :type metrics: str or list of strs\n",
    "  \"\"\"\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfJJdtWr05lm"
   },
   "source": [
    "Regression related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHbPhISzxXDZ"
   },
   "outputs": [],
   "source": [
    "def BasicRegressionPlotPerformance(model, X_test, y_test, metrics):\n",
    "  \"\"\"\n",
    "  metrics: mae, mse, r squared\n",
    "  \"\"\"\n",
    "\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yleXV5RbF4E-"
   },
   "outputs": [],
   "source": [
    "def MultiRegressionPlotPerformance(model, X_test, y_test, metrics):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZE9LGDmO9cN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7qI6pP808xJ"
   },
   "source": [
    "Classification related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AspcU-HoxvVj"
   },
   "outputs": [],
   "source": [
    "def BinaryClassificationPlotPerformance(model, X_test, y_test, metrics):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFNZYSX_0_U6"
   },
   "source": [
    "Image Classfication related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d6BWlXOx8Ox"
   },
   "outputs": [],
   "source": [
    "def ImageClassficationPlotPerformance(model, X_test, y_test, metrics):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv8OuMgM2QDX"
   },
   "source": [
    "# Input data comparison plots for different models (brainstorm ideas)\n",
    "1. The display of different datasets for different models will be different. Hence, we should handle them case by case.\n",
    "2. Users will provide a reference dataset and a test dataset. The reference dataset should reflect how an ideal and well-functioning dataset look like. We can then compare it with the test dataset.\n",
    "3. The main idea can be comparing each variable in the test dataset with each in the reference dataset. We can compare their distribution, summary(mean, standard deviation, max, min, etc.), data drift(look up the math formula or package available online), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiOWe1Dn4kQX"
   },
   "source": [
    "Time Series Forcasting related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1eOBj6l4fTV"
   },
   "outputs": [],
   "source": [
    "def TSPlotDistribution(ref_data, test_data):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GztOt8409r8i"
   },
   "outputs": [],
   "source": [
    "def TSDisplaySummary(ref_data, test_data):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ0cTzHw90V4"
   },
   "outputs": [],
   "source": [
    "def TSPlotDataDrift(ref_data, test_data):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xmpXJQl4s1v"
   },
   "source": [
    "Regression related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhKnWlIH4vcs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVotWR9G4vui"
   },
   "source": [
    "Classification related functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYXZdT-Q40UJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8pZYaa640nB"
   },
   "source": [
    "Image Classfication related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGBwF7g2441C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
